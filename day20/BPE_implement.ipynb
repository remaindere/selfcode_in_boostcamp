{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bpe_camper.ipynb","provenance":[{"file_id":"1Ue3inlDkAVEyLatLKGQ9iP3EIUG5dw9b","timestamp":1613658397331}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"koQ-w1sV34sz"},"source":["# Natural Language Processing\n","[BPE Paper Link](https://arxiv.org/pdf/1508.07909.pdf) "]},{"cell_type":"code","metadata":{"id":"9go8KbPe3s-L"},"source":["'''\r\n","자연어 처리에서의 BPE는 서브워드 분리(subword segmentation) 알고리즘임.\r\n","꽤나 많이 쓰인다고 함 \r\n","기존에 있던 단어를 분리한다는 의미.\r\n","BPE는 글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용함.\r\n","주어진 단어들을 모든 글자(chracters) 또는 유니코드(unicode) 단위로 단어 집합(vocabulary)를 만든 후에\r\n","가장 많이 등장하는 유니그램을 하나의 유니그램으로 통합하는 과정 반복함.\r\n","\r\n","기존의 접근\r\n","----------------------------------------------------------\r\n","주어진 dict\r\n","low : 5, lower : 2, newest : 6, widest : 3\r\n","\r\n","생성한 vocab\r\n","low, lower, newest, widest 임.\r\n","----------------------------------------------------------\r\n","\r\n","\r\n","BPE 알고리즘을 사용한 경우\r\n","----------------------------------------------------------\r\n","주어진 dict\r\n","l o w : 5,  l o w e r : 2,  n e w e s t : 6,  w i d e s t : 3\r\n","\r\n","생성한 vocab\r\n","l, o, w, e, r, n, w, s, t, i, d\r\n","----------------------------------------------------------\r\n","즉, 그냥 나온 알파벳만 가져온 것이 초기 vocabulary가 됨\r\n","그 이후로 미리 설정한 반복수만큼\r\n","가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합하는 과정을 반복함.\r\n","예시로, 위의 경우에서 1회차 수행은 다음과 같음\r\n","-----------------------------------------------------------\r\n","주어진 dict\r\n","l o w : 5, l o w e r : 2, n e w es t : 6, w i d es t : 3\r\n","\r\n","생성한 vocab\r\n","l, o, w, e, r, n, w, s, t, i, d, es\r\n","-----------------------------------------------------------\r\n","이 때의 반복 횟수에 따라, er이나 est와 같은 유니그램이 만들어지면서\r\n","예를 들어 newer 나 lowest와 같은 등록되어있지 않은 vocab이 출현한 경우에도\r\n","new+er 나 low+est와 같은 식으로 인코딩 할 수 있게 되고, OOV를 피해갈 수 있게됨.\r\n","\r\n","\r\n","보다 자세한 설명은 reference 참고 \r\n","ref : https://wikidocs.net/22592\r\n","'''\r\n","\r\n","from typing import List, Dict, Set\r\n","from itertools import chain\r\n","import re\r\n","from collections import defaultdict, Counter\r\n","\r\n","\r\n","def build_bpe(\r\n","        corpus: List[str],\r\n","        max_vocab_size: int\r\n",") -> List[int]:\r\n","    \"\"\" BPE Vocabulary Builder\r\n","    Implement vocabulary builder for byte pair encoding.\r\n","    Please sort your idx2word by subword length in descending manner.\r\n","\r\n","    Hint: Counter in collection library would be helpful\r\n","\r\n","    Note: If you convert sentences list to word frequence dictionary,\r\n","          building speed is enhanced significantly because duplicated words are\r\n","          preprocessed together\r\n","\r\n","    Arguments:\r\n","    corpus -- List of words to build vocab\r\n","    max_vocab_size -- The maximum size of vocab\r\n","\r\n","    Return:\r\n","    idx2word -- Subword list\r\n","    \"\"\"\r\n","    # Special tokens\r\n","    PAD = BytePairEncoding.PAD_token  # Index of <PAD> must be 0\r\n","    UNK = BytePairEncoding.UNK_token  # Index of <UNK> must be 1\r\n","    CLS = BytePairEncoding.CLS_token  # Index of <CLS> must be 2\r\n","    SEP = BytePairEncoding.SEP_token  # Index of <SEP> must be 3\r\n","    MSK = BytePairEncoding.MSK_token  # Index of <MSK> must be 4\r\n","    SPECIAL = [PAD, UNK, CLS, SEP, MSK]\r\n","\r\n","    WORD_END = BytePairEncoding.WORD_END  # Use this token as the end of a word\r\n","\r\n","\r\n","    def get_stats(vocab):\r\n","        stats = defaultdict(int)\r\n","        for word, freq in vocab.items(): #현재 step의 vocab 가져오기\r\n","            symbols = word.split() #띄어쓰기 된 녀석들 symbols에 넣기\r\n","            for i in range(len(symbols)-1):\r\n","                stats[symbols[i],symbols[i+1]] += freq #빈도수 측정\r\n","        return stats\r\n","\r\n","\r\n","    def replace_pair(most_freq, vocab_dict):\r\n","        replaced_vocab = {}\r\n","        bigram = re.escape(' '.join(most_freq))\r\n","        \r\n","        \r\n","        patt = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') #bigram이 b c일 때 a b c d , b c 같은 형태만 매칭하고  ab c d , b cd 같은 형태는 매칭하지 않을려고 사용\r\n","                                                          #ref:https://blog.hexabrain.net/205, 김규진_T1011 캠퍼님\r\n","        for word in vocab_dict :\r\n","            w_out = patt.sub(''.join(most_freq), word)\r\n","            replaced_vocab[w_out] = vocab_dict[word] #떨어져 있던 녀석들 붙여서 넘겨주기\r\n","        return replaced_vocab\r\n","\r\n","    #counter로 vocab별 출현횟수 가져오기\r\n","    vocabs = Counter(corpus)\r\n","    \r\n","    vocab_list = []\r\n","    for v, v_count in vocabs.items() :\r\n","        #알파벳 사이에 띄어쓰기 넣고(맨 끝에도), 해당 vocab가 끝났다는 의미의 _ 붙여놓기 \r\n","        vocab_list.append((str(' '.join(v[:-1])) + ' ' + str(v[-1] + ' _'), v_count))\r\n","    #print(vocab_list)\r\n","    vocab_dict = dict(vocab_list)\r\n","\r\n","    idx2word = []\r\n","\r\n","    #각 vocab별로 나온 token_list\r\n","    for token_list in vocab_dict.keys() :\r\n","        #띄어쓰기로 나눠놨던 각 토큰\r\n","        for token in token_list.split() :\r\n","            #만약 해당 토큰이 idx2word에 없다면 추가\r\n","            if token not in idx2word :\r\n","                idx2word.append(token)\r\n","    \r\n","    for i in range(max_vocab_size):\r\n","        #토큰쌍 출현 정보 가져오기 \r\n","        stats = get_stats(vocab_dict)\r\n","        #토큰쌍 출현 정보가 없대 ~ split 될게 없어 이미 다 끝났어 (max_vocab_size가 크게 설정되었음)\r\n","        if len(stats) == 0: \r\n","            break #그럼끝\r\n","        #print(stats) \r\n","        most_freq = max(stats, key=stats.get) #가장 빈번한 녀석\r\n","        #print(most_freq)\r\n","        #print(\"replace 이전 vocab_dict :\",vocab_dict)\r\n","        vocab_dict = replace_pair(most_freq, vocab_dict)\r\n","        #print(\"replace 이후 vocab_dict :\",vocab_dict)\r\n","        #가장 빈번한 토큰쌍 붙이고 idx2word에 저장\r\n","        idx2word.append(''.join(most_freq))\r\n","        #완료조건\r\n","        if len(idx2word) == max_vocab_size - 5 :\r\n","            break\r\n","\r\n","    #special 토큰들 넣고 긴 순서부터 길이에 맞춰 정렬 후 return\r\n","    idx2word = SPECIAL + sorted(idx2word, key=len, reverse=True)\r\n","    return idx2word"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XmiHfdPu_zm"},"source":[]},{"cell_type":"code","metadata":{"id":"ZG6-h8Wv5KWB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613730276139,"user_tz":-540,"elapsed":568,"user":{"displayName":"송광원","photoUrl":"","userId":"06583197538662483340"}},"outputId":"967bb448-ace0-4619-e4ed-ffa32a38a9ee"},"source":["#############################################\n","# Helper functions below. DO NOT MODIFY!    #\n","#############################################\n","\n","class BytePairEncoding(object):\n","    \"\"\" Byte Pair Encoding class\n","    We aren't gonna use this class for encoding. Because it is too slow......\n","    We will use sentence piece Google have made.\n","    Thus, this class is just for special token index reference.\n","    \"\"\"\n","    PAD_token = '<pad>'\n","    PAD_token_idx = 0\n","    UNK_token = '<unk>'\n","    UNK_token_idx = 1\n","    CLS_token = '<cls>'\n","    CLS_token_idx = 2\n","    SEP_token = '<sep>'\n","    SEP_token_idx = 3\n","    MSK_token = '<msk>'\n","    MSK_token_idx = 4\n","\n","    WORD_END = '_'\n","\n","    def __init__(self, corpus: List[List[str]], max_vocab_size: int) -> None:\n","        self.idx2word = build_bpe(corpus, max_vocab_size)\n","\n","    def encode(self, sentence: List[str]) -> List[int]:\n","        return encode(sentence, self.idx2word)\n","\n","    def decoder(self, tokens: List[int]) -> List[str]:\n","        return decode(tokens, self.idx2word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["======Building BPE Vocab Test Case======\n","The first test passed!\n","The second test passed!\n","['<pad>', '<unk>', '<cls>', '<sep>', '<msk>', 'abcde', 'abcd', 'abc', 'ab', 'a', 'b', 'c', 'd', 'e', '_']\n","The third test passed!\n","{'r', 'newest_', '<sep>', '_', 'est_', 'low', 'o', 'lo', 'e', 'i', 'd', 'l', 'est', 'new', '<unk>', 'n', 'ne', 't', 'es', '<msk>', '<cls>', 'w', '<pad>', 's'}\n","The forth test passed!\n","{'ab', 'aa', '<msk>', 'aaaaaaaa', 'aaaa', 'b', '<unk>', '<sep>', '<cls>', 'a', '_', '<pad>', 'abab'}\n","The fifth test passed!\n","['<pad>', '<unk>', '<cls>', '<sep>', '<msk>', 'abc_', 'bcd_', 'abc', 'bcd', 'bc', 'a', 'b', 'c', '_', 'd']\n","All 5 tests passed!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fJcvNEMqmDRe"},"source":[],"execution_count":null,"outputs":[]}]}